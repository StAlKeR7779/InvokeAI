from __future__ import annotations

from typing import TYPE_CHECKING, Optional, Tuple

import torch
from PIL import Image

from invokeai.backend.model_manager import BaseModelType
from invokeai.backend.stable_diffusion.extension_callback_type import ExtensionCallbackType
from invokeai.backend.stable_diffusion.extensions.base import ExtensionBase, callback

if TYPE_CHECKING:
    from invokeai.app.services.shared.invocation_context import InvocationContext
    from invokeai.backend.stable_diffusion.denoise_context import DenoiseContext

# fast latents preview matrix for sdxl
# generated by @StAlKeR7779
SDXL_LATENT_RGB_FACTORS = [
    #   R        G        B
    [0.3816, 0.4930, 0.5320],
    [-0.3753, 0.1631, 0.1739],
    [0.1770, 0.3588, -0.2048],
    [-0.4350, -0.2644, -0.4289],
]
SDXL_SMOOTH_MATRIX = [
    [0.0358, 0.0964, 0.0358],
    [0.0964, 0.4711, 0.0964],
    [0.0358, 0.0964, 0.0358],
]

# origingally adapted from code by @erucipe and @keturn here:
# https://discuss.huggingface.co/t/decoding-latents-to-rgb-without-upscaling/23204/7
# these updated numbers for v1.5 are from @torridgristle
SD1_5_LATENT_RGB_FACTORS = [
    #    R        G        B
    [0.3444, 0.1385, 0.0670],  # L1
    [0.1247, 0.4027, 0.1494],  # L2
    [-0.3192, 0.2513, 0.2103],  # L3
    [-0.1307, -0.1874, -0.7445],  # L4
]


class PreviewExt(ExtensionBase):
    def __init__(self, node_context: InvocationContext, base_model: BaseModelType):
        super().__init__()
        self.node_context = node_context
        self.base_model = base_model

    # do last so that all other changes shown
    @callback(ExtensionCallbackType.PRE_DENOISE_LOOP, order=1000)
    def initial_preview(self, ctx: DenoiseContext):
        image, scale = self.gen_latents_preview(ctx.latents, self.base_model)
        self.node_context.util.preview_callback(
            step=-1,
            total_steps=len(ctx.inputs.timesteps),
            order=ctx.scheduler.order,
            image=image,
            scale=scale,
        )

    # do last so that all other changes shown
    @callback(ExtensionCallbackType.POST_STEP, order=1000)
    def step_preview(self, ctx: DenoiseContext):
        if hasattr(ctx.step_output, "denoised"):
            predicted_original = ctx.step_output.denoised
        elif hasattr(ctx.step_output, "pred_original_sample"):
            predicted_original = ctx.step_output.pred_original_sample
        else:
            predicted_original = ctx.step_output.prev_sample

        image, scale = self.gen_latents_preview(predicted_original, self.base_model)
        self.node_context.util.preview_callback(
            step=ctx.step_index,
            total_steps=len(ctx.inputs.timesteps),
            order=ctx.scheduler.order,
            image=image,
            scale=scale,
        )

    @classmethod
    def _sample_to_lowres_estimated_image(
        cls, samples: torch.Tensor, latent_rgb_factors: torch.Tensor, smooth_matrix: Optional[torch.Tensor] = None
    ):
        latent_image = samples[0].permute(1, 2, 0) @ latent_rgb_factors

        if smooth_matrix is not None:
            latent_image = latent_image.unsqueeze(0).permute(3, 0, 1, 2)
            latent_image = torch.nn.functional.conv2d(latent_image, smooth_matrix.reshape((1, 1, 3, 3)), padding=1)
            latent_image = latent_image.permute(1, 2, 3, 0).squeeze(0)

        latents_ubyte = (
            ((latent_image + 1) / 2).clamp(0, 1).mul(0xFF).byte()  # change scale from -1..1 to 0..1  # to 0..255
        ).cpu()

        return Image.fromarray(latents_ubyte.numpy())

    @classmethod
    def gen_latents_preview(
        cls,
        sample: torch.Tensor,
        base_model: BaseModelType,
    ) -> Tuple[Image, int]:
        if base_model in [BaseModelType.StableDiffusionXL, BaseModelType.StableDiffusionXLRefiner]:
            sdxl_latent_rgb_factors = torch.tensor(SDXL_LATENT_RGB_FACTORS, dtype=sample.dtype, device=sample.device)
            sdxl_smooth_matrix = torch.tensor(SDXL_SMOOTH_MATRIX, dtype=sample.dtype, device=sample.device)
            image = cls._sample_to_lowres_estimated_image(sample, sdxl_latent_rgb_factors, sdxl_smooth_matrix)
        else:
            v1_5_latent_rgb_factors = torch.tensor(SD1_5_LATENT_RGB_FACTORS, dtype=sample.dtype, device=sample.device)
            image = cls._sample_to_lowres_estimated_image(sample, v1_5_latent_rgb_factors)

        return image, 8
